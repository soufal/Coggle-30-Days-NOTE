{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3fa75b-c942-446b-bb79-22db944cbf2f",
   "metadata": {},
   "source": [
    "### 30 Days of Spark\n",
    "\n",
    "#### 任务1：PySpark数据处理\n",
    "\n",
    "*    步骤1：使用Python链接Spark环境\n",
    "*    步骤2：创建dateframe数据\n",
    "*    步骤3：用spark执行以下逻辑：找到数据行数、列数\n",
    "*    步骤4：用spark筛选class为1的样本\n",
    "*    步骤5：用spark筛选language >90 或 math> 90的样本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973dfb5-de06-4a9a-b4d7-bdb6ae50f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、使用python链接Spark环境\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('pyspark') \\\n",
    "    .getOrCreate()\n",
    "# 原始数据 \n",
    "# 2、创建dataframe数据\n",
    "test = spark.createDataFrame([('001','1',100,87,67,83,98), ('002','2',87,81,90,83,83), ('003','3',86,91,83,89,63),\n",
    "                            ('004','2',65,87,94,73,88), ('005','1',76,62,89,81,98), ('006','3',84,82,85,73,99),\n",
    "                            ('007','3',56,76,63,72,87), ('008','1',55,62,46,78,71), ('009','2',63,72,87,98,64)],\n",
    "                             ['number','class','language','math','english','physic','chemical'])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcaa9de-1d02-4ca9-b04b-282ba1114d18",
   "metadata": {},
   "source": [
    "##### 找到数据的行数和列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3102b9e-ec31-4b1b-b040-1737bffddd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一\n",
    "column_len = len(test.columns)\n",
    "print(\"The length of DataFrame's columns is %s\" % column_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f224a4c-9eb3-431f-b589-102f945e7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一\n",
    "row_len = len(test.collect())\n",
    "print(\"The length of DataFrame's rows is %s\" % row_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc76d3-a7ee-4ae5-ad57-d757c74d6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二\n",
    "shape = (test.count(), len(test.columns))\n",
    "\n",
    "print(\"The length of DataFrame's rows is %s\" % shape[0])\n",
    "print(\"The length of DataFrame's columns is %s\" % shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051cb5a-6e4d-4347-bd9c-79bea241ae2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d91095-874a-4826-9699-5f54957c0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用spark筛选class为1的样本\n",
    "test.filter(test['class'] == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c7d42-be23-467b-bb54-0eed2427401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用spark筛选language>90 或math>90的样本\n",
    "test.filter((test['language'] > 90) | (test['math'] > 90)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5da766-53ad-4ff6-a584-6006b7069633",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "\n",
    "#### 任务2：PySpark数据统计\n",
    "\n",
    "* 步骤1：读取文件https://cdn.coggle.club/Pokemon.csv\n",
    "* 步骤2：将读取的进行保存，表头也需要保存\n",
    "* 步骤3：分析每列的类型，取值个数\n",
    "* 步骤4：分析每列是否包含缺失值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0c0d5-6139-433d-9f0c-129189680d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "# 读取文件\n",
    "spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')\n",
    "\n",
    "# 将读取的进行保存\n",
    "df = spark.read.csv(\"file://\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "df = df.withColumnRenamed('Sp. Def', 'Sp Def')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cff14a-3cbe-4fdd-9acf-423872e2f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa712b-4a9d-4c82-ac45-cebb6eb41cbd",
   "metadata": {},
   "source": [
    "##### 分析每一列的类型和取值个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194a853-a354-4f6b-98bd-d91ab4e680d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fd600-6158-429b-a815-e7e6918df206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb5682-b73a-4a8e-b22c-baabbf421cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Name').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f981cf-9dd7-4754-b297-8f21d733fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一：以去重的思想去分析列中的取值个数\n",
    "# 可采用两种方法\n",
    "\n",
    "# df.select('Name').drop_duplicates().count()\n",
    "\n",
    "df.select('Name').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d20aec-576a-4f92-a05c-05cd4184c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca068a-5a8c-48d0-b967-2a7d2cb14cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6ad80-0396-480d-9e0e-b892452667dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in columns_list:\n",
    "    value = df.select(i).drop_duplicates().count()\n",
    "    print(\"列 %s 的取值为：%s\" % (i, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb1be7-dbea-4646-b452-e9e4454b4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二：使用聚合函数 countDistinct\n",
    "import pyspark.sql.functions as F\n",
    "for i in columns_list:\n",
    "    print(df.agg(F.countDistinct(i).alias(i)).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f89a3f-766a-4a42-95ab-311386e94a80",
   "metadata": {},
   "source": [
    "> 会发现上面的两个结果中，对于列“Type 2”的结果有所不同， 检查数据后发现是因为“Type 2”中包含有却是之的数据，在第一种方法中，会将空值“NULL”当作一个值去统计，而使用`countDisinct`函数，他会排除出空值数据后再进行统计。\n",
    "> 下面先分析每列中是否包含有缺失值，然后再重新使用方法一统计。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69dcf6d-7fc9-4355-9179-f360829cd1c8",
   "metadata": {},
   "source": [
    "##### 分析每列是否包含缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f38457-29ae-4052-865e-49eb763802fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加对每一列进行去重处理后再统计取值\n",
    "for i in columns_list:\n",
    "    value = df.select(i).dropna().drop_duplicates().count()\n",
    "    print(\"列 %s 的取值为：%s\" % (i, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282d716-d130-4162-8186-01b3906e42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计每列数据缺失占比情况\n",
    "df.agg(*[(1 - (F.count(c) / F.count('*'))).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c280e8-5e00-41c9-8ff5-18c43b9595cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析每列中缺失值个数\n",
    "df_agg = df.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14853a8a-4db7-45bc-9d1b-c3d1798873ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be5f4c",
   "metadata": {},
   "source": [
    "---------------------------------------------------\n",
    "\n",
    "#### 任务三：\n",
    "\n",
    "* 步骤1：读取文件https://cdn.coggle.club/Pokemon.csv\n",
    "* 步骤2：学习groupby分组聚合的使用\n",
    "* 步骤3：学习agg分组聚合的使用\n",
    "* 步骤4：学习transform的使用\n",
    "* 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a398fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('pyspark') \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.addFile('Pokemon.csv')\n",
    "\n",
    "# 在windows下需要将file:// 改为file:///\n",
    "df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "df = df.withColumnRenamed('Sp. Def', 'Sp Def')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ebb81-78cd-4fe2-9eb9-7195661795b6",
   "metadata": {},
   "source": [
    "##### 步骤2：学习groupby分组聚合的使用\n",
    "\n",
    "PySpark DataFrame 还提供了一种使用常用方法拆分-应用-组合策略来处理分组数据的方法。按特定条件对数据进行分组，对每个组应用一个函数，然后将它们组合回 DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8984893-c421-4963-8f67-1ccf9eb12590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500e9b0-811b-418a-b13e-6222a318fbac",
   "metadata": {},
   "source": [
    "`df.groupby()`后可以使用自带基本统计功能的方法得到对应的结果（类似Pandas中GroupBy的用法）：\n",
    "\n",
    "其中可以指定返回某一列或某几列的统计结果。\n",
    "\n",
    "* `.count()`：返回每一组的数量，也就是行数。\n",
    "* `.mean()`：返回每一组的mean。\n",
    "* `.avg()`： 返回每一组的average。\n",
    "* `.sum()`：返回每一组的总和。\n",
    "* `.max()`：返回每一组的最大值。\n",
    "* `.min()`：返回每一组的最小值。\n",
    "\n",
    "\n",
    "> 均值(mean)是对恒定的真实值进行测量后，把测量偏离于真实值的所有值进行平均所得的结果；平均值(average)直接对一系列具有内部差异的数值进行的测量值进行的平均结果。均值是“观测值的平均”，平均值是“统计量的平均”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331cdd1-e9ac-44f9-bc82-24c5af5a5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组的数量\n",
    "df.groupby('Type 1').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e3804-42c1-452b-a14e-50d48a841581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组的平均值\n",
    "df.groupby('Type 1').mean(\"Total\", \"HP\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d52a1-304f-4df6-8c71-516946ae0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组的平均值\n",
    "df.groupby('Type 1').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb9413-5b7b-4f4f-8de1-0c34f2a77632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组各字段的最大值\n",
    "df.groupby('Type 1').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba53af2-2911-4970-9169-cc6ec8f565cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 返回指定字段的最大值\n",
    "df.groupby('Type 1').max(\"HP\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0600834-1e73-40d1-bd14-8d2f09ee70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组各字段的最小值\n",
    "df.groupby('Type 1').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c741a30-42de-41da-aff0-e735dc54c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组各字段的总和\n",
    "df.groupby('Type 1').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6a9f2-e830-49df-9ff9-f3a5c65c9614",
   "metadata": {},
   "source": [
    "##### 步骤3：学习agg分组聚合的使用\n",
    "\n",
    "使用 agg() 函数，可以一次计算多个聚合。即可以对多列使用不同的集合函数进行聚合\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213e41f-412a-419c-924d-641a0e1d13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count\n",
    "df.groupby('Type 1','Type 2').agg(count('HP').alias('总数'),\n",
    "                        max('HP').alias('最大HP值'),\n",
    "                        min('Attack').alias('最小攻击力')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f904c-ce5a-4676-9427-84122352838d",
   "metadata": {},
   "source": [
    "在 PySpark DataFrame 上，可以使用 where() 或 filter() 函数来过滤聚合数据的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e9603-af1c-4eff-9f5f-ecb73d0186fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count,col\n",
    "df.groupby('Type 1','Type 2').agg(count('HP').alias('总数'),\n",
    "                        max('HP').alias('最大HP值'),\n",
    "                        min('Attack').alias('最小攻击力')) \\\n",
    "                        .where(col('最小攻击力')>=40).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03842b9a-0582-45f3-882f-e0929127e15d",
   "metadata": {},
   "source": [
    "##### 步骤4：学习transform的使用\n",
    "\n",
    "返回一个新的 DataFrame。主要用于调用自定义的函数去处理DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505a8ec-071c-4d64-a857-b3f111f09cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdc682-525f-43da-b288-b50e1f1f1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cast_all_to_float(input_df):\n",
    "#     return input_df.select([(col(col_name) + 10) for col_name in input_df.columns])\n",
    "def sort_columns_asc(input_df):\n",
    "    return input_df.select(*sorted(input_df.columns))\n",
    "df.transform(sort_columns_asc).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f86767-cd01-42b9-a3dd-3a5a45becbc6",
   "metadata": {},
   "source": [
    "##### 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照Type 1分组 并统计HP的均值\n",
    "df.groupby('Type 1').mean('HP').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d9ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照Type 1分组 并统计HP的均值\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "df.groupby('Type 1').agg(mean('HP')).alias('Mean of HP').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca00693",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = {'aa' : {'Value':'nm','ed':'3'}}\n",
    "if 'aa' in aa:\n",
    "    print(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "type1_df = df.select('Type 1', 'HP')\n",
    "# type1_df.show()\n",
    "rows = type1_df.collect()\n",
    "# {Type 1: value:{HP:, count:}}\n",
    "result_dict = {}\n",
    "# d = [{'name': 'Alice', 'age': 1}]\n",
    "# output = spark.createDataFrame(d).collect()\n",
    "for row in rows:\n",
    "    if row['Type 1'] not in result_dict:\n",
    "        # key\n",
    "        result_dict[row['Type 1']] = {}\n",
    "\n",
    "        result_dict[row['Type 1']]['HP'] = row['HP']\n",
    "        result_dict[row['Type 1']]['count'] = 1\n",
    "    else:\n",
    "        result_dict[row['Type 1']]['HP'] += row['HP']\n",
    "        result_dict[row['Type 1']]['count'] += 1\n",
    "\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb58166",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = []\n",
    "for k, v in result_dict.items():\n",
    "    temp = {'Type 1':k,'mean':v['HP'] / v['count']}\n",
    "    # temp[k] = v['HP'] / v['count']\n",
    "    result_df.append(temp)\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1853cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = spark.createDataFrame(result_df).collect()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ceb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照Type 1分组 并统计HP的均值\n",
    "def com_mean(input_df):\n",
    "\n",
    "    return input_df.groupby('Type 1').mean('HP')\n",
    "\n",
    "df.transform(com_mean).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1782d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac940a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
