{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3fa75b-c942-446b-bb79-22db944cbf2f",
   "metadata": {},
   "source": [
    "### 30 Days of Spark\n",
    "\n",
    "#### 任务1：PySpark数据处理\n",
    "\n",
    "*    步骤1：使用Python链接Spark环境\n",
    "*    步骤2：创建dateframe数据\n",
    "*    步骤3：用spark执行以下逻辑：找到数据行数、列数\n",
    "*    步骤4：用spark筛选class为1的样本\n",
    "*    步骤5：用spark筛选language >90 或 math> 90的样本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973dfb5-de06-4a9a-b4d7-bdb6ae50f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、使用python链接Spark环境\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('pyspark') \\\n",
    "    .getOrCreate()\n",
    "# 原始数据 \n",
    "# 2、创建dataframe数据\n",
    "test = spark.createDataFrame([('001','1',100,87,67,83,98), ('002','2',87,81,90,83,83), ('003','3',86,91,83,89,63),\n",
    "                            ('004','2',65,87,94,73,88), ('005','1',76,62,89,81,98), ('006','3',84,82,85,73,99),\n",
    "                            ('007','3',56,76,63,72,87), ('008','1',55,62,46,78,71), ('009','2',63,72,87,98,64)],\n",
    "                             ['number','class','language','math','english','physic','chemical'])\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcaa9de-1d02-4ca9-b04b-282ba1114d18",
   "metadata": {},
   "source": [
    "##### 找到数据的行数和列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3102b9e-ec31-4b1b-b040-1737bffddd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一\n",
    "column_len = len(test.columns)\n",
    "print(\"The length of DataFrame's columns is %s\" % column_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f224a4c-9eb3-431f-b589-102f945e7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一\n",
    "row_len = len(test.collect())\n",
    "print(\"The length of DataFrame's rows is %s\" % row_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc76d3-a7ee-4ae5-ad57-d757c74d6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二\n",
    "shape = (test.count(), len(test.columns))\n",
    "\n",
    "print(\"The length of DataFrame's rows is %s\" % shape[0])\n",
    "print(\"The length of DataFrame's columns is %s\" % shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051cb5a-6e4d-4347-bd9c-79bea241ae2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d91095-874a-4826-9699-5f54957c0901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用spark筛选class为1的样本\n",
    "test.filter(test['class'] == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c7d42-be23-467b-bb54-0eed2427401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用spark筛选language>90 或math>90的样本\n",
    "test.filter((test['language'] > 90) | (test['math'] > 90)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5da766-53ad-4ff6-a584-6006b7069633",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "\n",
    "#### 任务2：PySpark数据统计\n",
    "\n",
    "* 步骤1：读取文件https://cdn.coggle.club/Pokemon.csv\n",
    "* 步骤2：将读取的进行保存，表头也需要保存\n",
    "* 步骤3：分析每列的类型，取值个数\n",
    "* 步骤4：分析每列是否包含缺失值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0c0d5-6139-433d-9f0c-129189680d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "# 读取文件\n",
    "spark.sparkContext.addFile('https://cdn.coggle.club/Pokemon.csv')\n",
    "\n",
    "# 将读取的进行保存\n",
    "df = spark.read.csv(\"file://\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "df = df.withColumnRenamed('Sp. Def', 'Sp Def')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cff14a-3cbe-4fdd-9acf-423872e2f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa712b-4a9d-4c82-ac45-cebb6eb41cbd",
   "metadata": {},
   "source": [
    "##### 分析每一列的类型和取值个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194a853-a354-4f6b-98bd-d91ab4e680d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fd600-6158-429b-a815-e7e6918df206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb5682-b73a-4a8e-b22c-baabbf421cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Name').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f981cf-9dd7-4754-b297-8f21d733fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法一：以去重的思想去分析列中的取值个数\n",
    "# 可采用两种方法\n",
    "\n",
    "# df.select('Name').drop_duplicates().count()\n",
    "\n",
    "df.select('Name').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "16d20aec-576a-4f92-a05c-05cd4184c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "06ca068a-5a8c-48d0-b967-2a7d2cb14cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'Type1',\n",
       " 'Type2',\n",
       " 'Total',\n",
       " 'HP',\n",
       " 'Attack',\n",
       " 'Defense',\n",
       " 'SpAtk',\n",
       " 'SpDef',\n",
       " 'Speed',\n",
       " 'Generation',\n",
       " 'Legendary']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5ee6ad80-0396-480d-9e0e-b892452667dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列 Name 的取值为：799\n",
      "列 Type1 的取值为：18\n",
      "列 Type2 的取值为：19\n",
      "列 Total 的取值为：200\n",
      "列 HP 的取值为：94\n",
      "列 Attack 的取值为：111\n",
      "列 Defense 的取值为：103\n",
      "列 SpAtk 的取值为：105\n",
      "列 SpDef 的取值为：92\n",
      "列 Speed 的取值为：108\n",
      "列 Generation 的取值为：6\n",
      "列 Legendary 的取值为：2\n"
     ]
    }
   ],
   "source": [
    "for i in columns_list:\n",
    "    value = df.select(i).drop_duplicates().count()\n",
    "    print(\"列 %s 的取值为：%s\" % (i, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "48fb1be7-dbea-4646-b452-e9e4454b4cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Name=799)]\n",
      "[Row(Type1=18)]\n",
      "[Row(Type2=18)]\n",
      "[Row(Total=200)]\n",
      "[Row(HP=94)]\n",
      "[Row(Attack=111)]\n",
      "[Row(Defense=103)]\n",
      "[Row(SpAtk=105)]\n",
      "[Row(SpDef=92)]\n",
      "[Row(Speed=108)]\n",
      "[Row(Generation=6)]\n",
      "[Row(Legendary=2)]\n"
     ]
    }
   ],
   "source": [
    "# 方法二：使用聚合函数 countDistinct\n",
    "import pyspark.sql.functions as F\n",
    "for i in columns_list:\n",
    "    print(df.agg(F.countDistinct(i).alias(i)).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f89a3f-766a-4a42-95ab-311386e94a80",
   "metadata": {},
   "source": [
    "> 会发现上面的两个结果中，对于列“Type 2”的结果有所不同， 检查数据后发现是因为“Type 2”中包含有缺失值的数据，在第一种方法中，会将空值“NULL”当作一个值去统计，而使用`countDisinct`函数，他会排除出空值数据后再进行统计。\n",
    "> 下面先分析每列中是否包含有缺失值，然后再重新使用方法一统计。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69dcf6d-7fc9-4355-9179-f360829cd1c8",
   "metadata": {},
   "source": [
    "##### 分析每列是否包含缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f38457-29ae-4052-865e-49eb763802fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加对每一列进行去重处理后再统计取值\n",
    "for i in columns_list:\n",
    "    value = df.select(i).dropna().drop_duplicates().count()\n",
    "    print(\"列 %s 的取值为：%s\" % (i, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282d716-d130-4162-8186-01b3906e42d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计每列数据缺失占比情况\n",
    "df.agg(*[(1 - (F.count(c) / F.count('*'))).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83c280e8-5e00-41c9-8ff5-18c43b9595cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析每列中缺失值个数\n",
    "df_agg = df.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "14853a8a-4db7-45bc-9d1b-c3d1798873ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+-----+---+------+-------+-----+-----+-----+----------+---------+\n",
      "|Name|Type1|Type2|Total| HP|Attack|Defense|SpAtk|SpDef|Speed|Generation|Legendary|\n",
      "+----+-----+-----+-----+---+------+-------+-----+-----+-----+----------+---------+\n",
      "|   0|    0|  386|    0|  0|     0|      0|    0|    0|    0|         0|        0|\n",
      "+----+-----+-----+-----+---+------+-------+-----+-----+-----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be5f4c",
   "metadata": {},
   "source": [
    "---------------------------------------------------\n",
    "\n",
    "#### 任务三：\n",
    "\n",
    "* 步骤1：读取文件https://cdn.coggle.club/Pokemon.csv\n",
    "* 步骤2：学习groupby分组聚合的使用\n",
    "* 步骤3：学习agg分组聚合的使用\n",
    "* 步骤4：学习transform的使用\n",
    "* 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a398fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('pyspark') \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.addFile('Pokemon.csv')\n",
    "\n",
    "# 在windows下需要将file:// 改为file:///\n",
    "df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "df = df.withColumnRenamed('Sp. Def', 'Sp Def')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ebb81-78cd-4fe2-9eb9-7195661795b6",
   "metadata": {},
   "source": [
    "##### 步骤2：学习groupby分组聚合的使用\n",
    "\n",
    "PySpark DataFrame 还提供了一种使用常用方法拆分-应用-组合策略来处理分组数据的方法。按特定条件对数据进行分组，对每个组应用一个函数，然后将它们组合回 DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8984893-c421-4963-8f67-1ccf9eb12590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500e9b0-811b-418a-b13e-6222a318fbac",
   "metadata": {},
   "source": [
    "`df.groupby()`后可以使用自带基本统计功能的方法得到对应的结果（类似Pandas中GroupBy的用法）：\n",
    "\n",
    "其中可以指定返回某一列或某几列的统计结果。\n",
    "\n",
    "* `.count()`：返回每一组的数量，也就是行数。\n",
    "* `.mean()`：返回每一组的mean。\n",
    "* `.avg()`： 返回每一组的average。\n",
    "* `.sum()`：返回每一组的总和。\n",
    "* `.max()`：返回每一组的最大值。\n",
    "* `.min()`：返回每一组的最小值。\n",
    "\n",
    "\n",
    "> 均值(mean)是对恒定的真实值进行测量后，把测量偏离于真实值的所有值进行平均所得的结果；平均值(average)直接对一系列具有内部差异的数值进行的测量值进行的平均结果。均值是“观测值的平均”，平均值是“统计量的平均”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331cdd1-e9ac-44f9-bc82-24c5af5a5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组的数量\n",
    "df.groupby('Type 1').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e3804-42c1-452b-a14e-50d48a841581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组的平均值\n",
    "df.groupby('Type 1').mean(\"Total\", \"HP\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d52a1-304f-4df6-8c71-516946ae0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组的平均值\n",
    "df.groupby('Type 1').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb9413-5b7b-4f4f-8de1-0c34f2a77632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组各字段的最大值\n",
    "df.groupby('Type 1').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba53af2-2911-4970-9169-cc6ec8f565cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 返回指定字段的最大值\n",
    "df.groupby('Type 1').max(\"HP\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0600834-1e73-40d1-bd14-8d2f09ee70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组各字段的最小值\n",
    "df.groupby('Type 1').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c741a30-42de-41da-aff0-e735dc54c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照某一个字段分组 并统计各组各字段的总和\n",
    "df.groupby('Type 1').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6a9f2-e830-49df-9ff9-f3a5c65c9614",
   "metadata": {},
   "source": [
    "##### 步骤3：学习agg分组聚合的使用\n",
    "\n",
    "使用 agg() 函数，可以一次计算多个聚合。即可以对多列使用不同的集合函数进行聚合\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213e41f-412a-419c-924d-641a0e1d13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count\n",
    "df.groupby('Type 1','Type 2').agg(count('HP').alias('总数'),\n",
    "                        max('HP').alias('最大HP值'),\n",
    "                        min('Attack').alias('最小攻击力')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f904c-ce5a-4676-9427-84122352838d",
   "metadata": {},
   "source": [
    "在 PySpark DataFrame 上，可以使用 where() 或 filter() 函数来过滤聚合数据的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e9603-af1c-4eff-9f5f-ecb73d0186fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count,col\n",
    "df.groupby('Type 1','Type 2').agg(count('HP').alias('总数'),\n",
    "                        max('HP').alias('最大HP值'),\n",
    "                        min('Attack').alias('最小攻击力')) \\\n",
    "                        .where(col('最小攻击力')>=40).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03842b9a-0582-45f3-882f-e0929127e15d",
   "metadata": {},
   "source": [
    "##### 步骤4：学习transform的使用\n",
    "\n",
    "返回一个新的 DataFrame。主要用于调用自定义的函数去处理DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505a8ec-071c-4d64-a857-b3f111f09cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdc682-525f-43da-b288-b50e1f1f1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cast_all_to_float(input_df):\n",
    "#     return input_df.select([(col(col_name) + 10) for col_name in input_df.columns])\n",
    "def sort_columns_asc(input_df):\n",
    "    return input_df.select(*sorted(input_df.columns))\n",
    "df.transform(sort_columns_asc).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f86767-cd01-42b9-a3dd-3a5a45becbc6",
   "metadata": {},
   "source": [
    "##### 步骤5：使用groupby、agg、transform，统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照Type 1分组 并统计HP的均值\n",
    "df.groupby('Type 1').mean('HP').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d9ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照Type 1分组 并统计HP的均值\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "df.groupby('Type 1').agg(mean('HP')).alias('Mean of HP').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f3bc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Grass': {'HP': 4709, 'count': 70}, 'Fire': {'HP': 3635, 'count': 52}, 'Water': {'HP': 8071, 'count': 112}, 'Bug': {'HP': 3925, 'count': 69}, 'Normal': {'HP': 7573, 'count': 98}, 'Poison': {'HP': 1883, 'count': 28}, 'Electric': {'HP': 2631, 'count': 44}, 'Ground': {'HP': 2361, 'count': 32}, 'Fairy': {'HP': 1260, 'count': 17}, 'Fighting': {'HP': 1886, 'count': 27}, 'Psychic': {'HP': 4026, 'count': 57}, 'Rock': {'HP': 2876, 'count': 44}, 'Ghost': {'HP': 2062, 'count': 32}, 'Ice': {'HP': 1728, 'count': 24}, 'Dragon': {'HP': 2666, 'count': 32}, 'Dark': {'HP': 2071, 'count': 31}, 'Steel': {'HP': 1761, 'count': 27}, 'Flying': {'HP': 283, 'count': 4}}\n"
     ]
    }
   ],
   "source": [
    "type1_df = df.select('Type 1', 'HP')\n",
    "# type1_df.show()\n",
    "rows = type1_df.collect()\n",
    "# {Type 1: value:{HP:, count:}}\n",
    "result_dict = {}\n",
    "# d = [{'name': 'Alice', 'age': 1}]\n",
    "# output = spark.createDataFrame(d).collect()\n",
    "for row in rows:\n",
    "    if row['Type 1'] not in result_dict:\n",
    "        # key\n",
    "        result_dict[row['Type 1']] = {}\n",
    "\n",
    "        result_dict[row['Type 1']]['HP'] = row['HP']\n",
    "        result_dict[row['Type 1']]['count'] = 1\n",
    "    else:\n",
    "        result_dict[row['Type 1']]['HP'] += row['HP']\n",
    "        result_dict[row['Type 1']]['count'] += 1\n",
    "\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb58166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Type 1': 'Grass', 'mean': 67.27142857142857},\n",
       " {'Type 1': 'Fire', 'mean': 69.90384615384616},\n",
       " {'Type 1': 'Water', 'mean': 72.0625},\n",
       " {'Type 1': 'Bug', 'mean': 56.88405797101449},\n",
       " {'Type 1': 'Normal', 'mean': 77.27551020408163},\n",
       " {'Type 1': 'Poison', 'mean': 67.25},\n",
       " {'Type 1': 'Electric', 'mean': 59.79545454545455},\n",
       " {'Type 1': 'Ground', 'mean': 73.78125},\n",
       " {'Type 1': 'Fairy', 'mean': 74.11764705882354},\n",
       " {'Type 1': 'Fighting', 'mean': 69.85185185185185},\n",
       " {'Type 1': 'Psychic', 'mean': 70.63157894736842},\n",
       " {'Type 1': 'Rock', 'mean': 65.36363636363636},\n",
       " {'Type 1': 'Ghost', 'mean': 64.4375},\n",
       " {'Type 1': 'Ice', 'mean': 72.0},\n",
       " {'Type 1': 'Dragon', 'mean': 83.3125},\n",
       " {'Type 1': 'Dark', 'mean': 66.80645161290323},\n",
       " {'Type 1': 'Steel', 'mean': 65.22222222222223},\n",
       " {'Type 1': 'Flying', 'mean': 70.75}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = []\n",
    "for k, v in result_dict.items():\n",
    "    temp = {'Type 1':k,'mean':v['HP'] / v['count']}\n",
    "    # temp[k] = v['HP'] / v['count']\n",
    "    result_df.append(temp)\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1853cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|  Type 1|             mean|\n",
      "+--------+-----------------+\n",
      "|   Grass|67.27142857142857|\n",
      "|    Fire|69.90384615384616|\n",
      "|   Water|          72.0625|\n",
      "|     Bug|56.88405797101449|\n",
      "|  Normal|77.27551020408163|\n",
      "|  Poison|            67.25|\n",
      "|Electric|59.79545454545455|\n",
      "|  Ground|         73.78125|\n",
      "|   Fairy|74.11764705882354|\n",
      "|Fighting|69.85185185185185|\n",
      "| Psychic|70.63157894736842|\n",
      "|    Rock|65.36363636363636|\n",
      "|   Ghost|          64.4375|\n",
      "|     Ice|             72.0|\n",
      "|  Dragon|          83.3125|\n",
      "|    Dark|66.80645161290323|\n",
      "|   Steel|65.22222222222223|\n",
      "|  Flying|            70.75|\n",
      "+--------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "output = spark.createDataFrame(result_df)\n",
    "print(output.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079ceb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|  Type 1|             mean|\n",
      "+--------+-----------------+\n",
      "|   Grass|67.27142857142857|\n",
      "|    Fire|69.90384615384616|\n",
      "|   Water|          72.0625|\n",
      "|     Bug|56.88405797101449|\n",
      "|  Normal|77.27551020408163|\n",
      "|  Poison|            67.25|\n",
      "|Electric|59.79545454545455|\n",
      "|  Ground|         73.78125|\n",
      "|   Fairy|74.11764705882354|\n",
      "|Fighting|69.85185185185185|\n",
      "| Psychic|70.63157894736842|\n",
      "|    Rock|65.36363636363636|\n",
      "|   Ghost|          64.4375|\n",
      "|     Ice|             72.0|\n",
      "|  Dragon|          83.3125|\n",
      "|    Dark|66.80645161290323|\n",
      "|   Steel|65.22222222222223|\n",
      "|  Flying|            70.75|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 按照Type 1分组 并统计HP的均值\n",
    "# 自己实现一个分组并计算均值\n",
    "def com_mean(input_df):\n",
    "    type1_df = df.select('Type 1', 'HP')\n",
    "    # type1_df.show()\n",
    "    rows = type1_df.collect()\n",
    "    # {Type 1: value:{HP:, count:}}\n",
    "    result_dict = {}\n",
    "    # d = [{'name': 'Alice', 'age': 1}]\n",
    "    # output = spark.createDataFrame(d).collect()\n",
    "    for row in rows:\n",
    "        if row['Type 1'] not in result_dict:\n",
    "            # key\n",
    "            result_dict[row['Type 1']] = {}\n",
    "\n",
    "            result_dict[row['Type 1']]['HP'] = row['HP']\n",
    "            result_dict[row['Type 1']]['count'] = 1\n",
    "        else:\n",
    "            result_dict[row['Type 1']]['HP'] += row['HP']\n",
    "            result_dict[row['Type 1']]['count'] += 1\n",
    "    result_df = []\n",
    "    for k, v in result_dict.items():\n",
    "        temp = {'Type 1':k,'mean':v['HP'] / v['count']}\n",
    "        # temp[k] = v['HP'] / v['count']\n",
    "        result_df.append(temp)\n",
    "    output = spark.createDataFrame(result_df)\n",
    "    return output\n",
    "\n",
    "df.transform(com_mean).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6c49b-27a7-4e4d-8513-b2fd230e0a04",
   "metadata": {
    "tags": []
   },
   "source": [
    "---------------------------------------------------\n",
    "\n",
    "#### 任务四：SparkSQL基础语法\n",
    "\n",
    "* 步骤1：使用Spark SQL完成任务1里面的数据筛选\n",
    "    * 用spark筛选class为1的样本\n",
    "    * 用spark筛选language >90 或 math> 90的样本\n",
    "* 步骤2：使用Spark SQL完成任务2里面的统计（列可以不统计）\n",
    "    * 分析每列的类型，取值个数\n",
    "    * 分析每列是否包含缺失值\n",
    "* 步骤3：使用Spark SQL完成任务3的分组统计\n",
    "    * 统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c622692-5cfd-4a53-8fdf-586bdea27dee",
   "metadata": {},
   "source": [
    "##### 步骤1：使用Spark SQL完成任务1里面的数据筛选\n",
    "* 用spark筛选class为1的样本\n",
    "* 用spark筛选language >90 或 math> 90的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "716b6723-68e6-42a5-becd-95d4da477d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2ad27d-529c-4bea-bb34-fcb0403f0dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务一数据\n",
    "task_1 = spark.createDataFrame([('001','1',100,87,67,83,98), ('002','2',87,81,90,83,83), ('003','3',86,91,83,89,63),\n",
    "                            ('004','2',65,87,94,73,88), ('005','1',76,62,89,81,98), ('006','3',84,82,85,73,99),\n",
    "                            ('007','3',56,76,63,72,87), ('008','1',55,62,46,78,71), ('009','2',63,72,87,98,64)],\n",
    "                             ['number','class','language','math','english','physic','chemical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20adf9c5-041b-48fc-9def-dbc750920e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将DataFrame注册为sql临时表\n",
    "task_1.createOrReplaceTempView(\"task_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd807cbc-0c53-473e-b536-a2c957f5e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   002|    2|      87|  81|     90|    83|      83|\n",
      "|   003|    3|      86|  91|     83|    89|      63|\n",
      "|   004|    2|      65|  87|     94|    73|      88|\n",
      "|   005|    1|      76|  62|     89|    81|      98|\n",
      "|   006|    3|      84|  82|     85|    73|      99|\n",
      "|   007|    3|      56|  76|     63|    72|      87|\n",
      "|   008|    1|      55|  62|     46|    78|      71|\n",
      "|   009|    2|      63|  72|     87|    98|      64|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM task_1\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5943501-a0ec-4eab-b5e0-46f7de719357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   005|    1|      76|  62|     89|    81|      98|\n",
      "|   008|    1|      55|  62|     46|    78|      71|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 筛选class为1的样本\n",
    "sql1 = spark.sql(\"SELECT * FROM task_1 WHERE class=1\")\n",
    "sql1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "219637ac-4ea5-46ad-a869-502b7c9ff675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+----+-------+------+--------+\n",
      "|number|class|language|math|english|physic|chemical|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "|   001|    1|     100|  87|     67|    83|      98|\n",
      "|   003|    3|      86|  91|     83|    89|      63|\n",
      "+------+-----+--------+----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 筛选language >90 或 math> 90的样本\n",
    "sql2 = spark.sql(\"SELECT * FROM task_1 WHERE language>90 OR math>90\")\n",
    "sql2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd3a65-678f-4ac3-8005-fc44891a4e24",
   "metadata": {},
   "source": [
    "##### 步骤2：使用Spark SQL完成任务2里面的统计（列可以不统计）\n",
    "* 分析每列的类型，取值个数\n",
    "* 分析每列是否包含缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1a486e-ad6c-4cf4-a374-b9778aeb2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('Pokemon.csv', header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77a4777b-0a33-4358-8716-c8b4713b850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+-------+-------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp. Atk|Sp. Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+-------+-------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|     65|     65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|     80|     80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|    100|    100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|    122|    120|   80|         1|    false|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|     60|     50|   65|         1|    false|\n",
      "|          Charmeleon|  Fire|  null|  405| 58|    64|     58|     80|     65|   80|         1|    false|\n",
      "|           Charizard|  Fire|Flying|  534| 78|    84|     78|    109|     85|  100|         1|    false|\n",
      "|CharizardMega Cha...|  Fire|Dragon|  634| 78|   130|    111|    130|     85|  100|         1|    false|\n",
      "|CharizardMega Cha...|  Fire|Flying|  634| 78|   104|     78|    159|    115|  100|         1|    false|\n",
      "|            Squirtle| Water|  null|  314| 44|    48|     65|     50|     64|   43|         1|    false|\n",
      "|           Wartortle| Water|  null|  405| 59|    63|     80|     65|     80|   58|         1|    false|\n",
      "|           Blastoise| Water|  null|  530| 79|    83|    100|     85|    105|   78|         1|    false|\n",
      "|BlastoiseMega Bla...| Water|  null|  630| 79|   103|    120|    135|    115|   78|         1|    false|\n",
      "|            Caterpie|   Bug|  null|  195| 45|    30|     35|     20|     20|   45|         1|    false|\n",
      "|             Metapod|   Bug|  null|  205| 50|    20|     55|     25|     25|   30|         1|    false|\n",
      "|          Butterfree|   Bug|Flying|  395| 60|    45|     50|     90|     80|   70|         1|    false|\n",
      "|              Weedle|   Bug|Poison|  195| 40|    35|     30|     20|     20|   50|         1|    false|\n",
      "|              Kakuna|   Bug|Poison|  205| 45|    25|     50|     25|     25|   35|         1|    false|\n",
      "|            Beedrill|   Bug|Poison|  395| 65|    90|     40|     45|     80|   75|         1|    false|\n",
      "|BeedrillMega Beed...|   Bug|Poison|  495| 65|   150|     40|     15|     80|  145|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+-------+-------+-----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b3d6623-3857-465b-a116-2dd7467e42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉字段名中的空格,防止在SQL中识别到空格出错\n",
    "df = df.withColumnRenamed('Type 1', 'Type1')\n",
    "df = df.withColumnRenamed('Type 2', 'Type2')\n",
    "df = df.withColumnRenamed('Sp. Atk', 'SpAtk')\n",
    "df = df.withColumnRenamed('Sp. Def', 'SpDef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e639cd5-6a77-44ae-97e6-e5a131b5ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将DataFrame注册为sql临时表\n",
    "df.createOrReplaceTempView(\"Pokemon\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "176c3ad2-664f-4b63-a4cf-766e69e6b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+-----+---+------+-------+-----+-----+-----+----------+---------+\n",
      "|                Name|Type1| Type2|Total| HP|Attack|Defense|SpAtk|SpDef|Speed|Generation|Legendary|\n",
      "+--------------------+-----+------+-----+---+------+-------+-----+-----+-----+----------+---------+\n",
      "|           Bulbasaur|Grass|Poison|  318| 45|    49|     49|   65|   65|   45|         1|    false|\n",
      "|             Ivysaur|Grass|Poison|  405| 60|    62|     63|   80|   80|   60|         1|    false|\n",
      "|            Venusaur|Grass|Poison|  525| 80|    82|     83|  100|  100|   80|         1|    false|\n",
      "|VenusaurMega Venu...|Grass|Poison|  625| 80|   100|    123|  122|  120|   80|         1|    false|\n",
      "|          Charmander| Fire|  null|  309| 39|    52|     43|   60|   50|   65|         1|    false|\n",
      "|          Charmeleon| Fire|  null|  405| 58|    64|     58|   80|   65|   80|         1|    false|\n",
      "|           Charizard| Fire|Flying|  534| 78|    84|     78|  109|   85|  100|         1|    false|\n",
      "|CharizardMega Cha...| Fire|Dragon|  634| 78|   130|    111|  130|   85|  100|         1|    false|\n",
      "|CharizardMega Cha...| Fire|Flying|  634| 78|   104|     78|  159|  115|  100|         1|    false|\n",
      "|            Squirtle|Water|  null|  314| 44|    48|     65|   50|   64|   43|         1|    false|\n",
      "|           Wartortle|Water|  null|  405| 59|    63|     80|   65|   80|   58|         1|    false|\n",
      "|           Blastoise|Water|  null|  530| 79|    83|    100|   85|  105|   78|         1|    false|\n",
      "|BlastoiseMega Bla...|Water|  null|  630| 79|   103|    120|  135|  115|   78|         1|    false|\n",
      "|            Caterpie|  Bug|  null|  195| 45|    30|     35|   20|   20|   45|         1|    false|\n",
      "|             Metapod|  Bug|  null|  205| 50|    20|     55|   25|   25|   30|         1|    false|\n",
      "|          Butterfree|  Bug|Flying|  395| 60|    45|     50|   90|   80|   70|         1|    false|\n",
      "|              Weedle|  Bug|Poison|  195| 40|    35|     30|   20|   20|   50|         1|    false|\n",
      "|              Kakuna|  Bug|Poison|  205| 45|    25|     50|   25|   25|   35|         1|    false|\n",
      "|            Beedrill|  Bug|Poison|  395| 65|    90|     40|   45|   80|   75|         1|    false|\n",
      "|BeedrillMega Beed...|  Bug|Poison|  495| 65|   150|     40|   15|   80|  145|         1|    false|\n",
      "+--------------------+-----+------+-----+---+------+-------+-----+-----+-----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql3 = spark.sql(\"SELECT * FROM Pokemon\")\n",
    "sql3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f12a5d75-60ef-46ba-bfa3-7143cffb9261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|      Name|   string|   null|\n",
      "|     Type1|   string|   null|\n",
      "|     Type2|   string|   null|\n",
      "|     Total|      int|   null|\n",
      "|        HP|      int|   null|\n",
      "|    Attack|      int|   null|\n",
      "|   Defense|      int|   null|\n",
      "|     SpAtk|      int|   null|\n",
      "|     SpDef|      int|   null|\n",
      "|     Speed|      int|   null|\n",
      "|Generation|      int|   null|\n",
      "| Legendary|  boolean|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分析每列的类型\n",
    "sql4 = spark.sql(\"DESCRIBE TABLE Pokemon\")\n",
    "sql4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "703f41a1-f9e7-445f-9e4a-6c6d6c68b808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'Type1',\n",
       " 'Type2',\n",
       " 'Total',\n",
       " 'HP',\n",
       " 'Attack',\n",
       " 'Defense',\n",
       " 'SpAtk',\n",
       " 'SpDef',\n",
       " 'Speed',\n",
       " 'Generation',\n",
       " 'Legendary']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "832e0c6d-bd95-4bfd-bb80-a82148d3b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+-----+---+------+-------+-----+-----+-----+----------+---------+\n",
      "|Name|Type1|Type2|Total| HP|Attack|Defense|SpAtk|SpDef|Speed|Generation|Legendary|\n",
      "+----+-----+-----+-----+---+------+-------+-----+-----+-----+----------+---------+\n",
      "| 799|   18|   18|  200| 94|   111|    103|  105|   92|  108|         6|        2|\n",
      "+----+-----+-----+-----+---+------+-------+-----+-----+-----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分析每列的取值个数\n",
    "sql5 = spark.sql(\"SELECT COUNT(DISTINCT Name) as Name, COUNT(DISTINCT Type1) as Type1,  COUNT(DISTINCT Type2) as Type2, COUNT(DISTINCT Total) as Total, \\\n",
    "                 COUNT(DISTINCT HP) as HP, COUNT(DISTINCT Attack) as Attack, COUNT(DISTINCT Defense) as Defense, COUNT(DISTINCT SpAtk) as SpAtk, COUNT(DISTINCT SpDef) as SpDef,  \\\n",
    "                 COUNT(DISTINCT Speed) as Speed, COUNT(DISTINCT Generation) as Generation,  COUNT(DISTINCT Legendary) as Legendary FROM Pokemon\")\n",
    "sql5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2d0f1dd8-9071-464e-a8db-88b597c495be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     386|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分析每列是否包含缺失值\n",
    "sql6 = spark.sql(\"select count(*) from Pokemon where Type2 is null\")\n",
    "sql6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cf5d9bcc-f831-4c79-9571-93cef08d058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     386|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分析每列是否包含缺失值\n",
    "sql7 = spark.sql(\"select count(*) from Pokemon where isnull(Type2)='true'\")\n",
    "sql7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8866a5f-c69b-4eb3-a71a-608d5c40158b",
   "metadata": {},
   "source": [
    "##### 步骤3：使用Spark SQL完成任务3的分组统计\n",
    "* 统计数据在Type 1分组下 HP的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eedac443-91fa-407b-b0ef-39a5b3e8849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         mean(HP)|\n",
      "+-----------------+\n",
      "|          72.0625|\n",
      "|            67.25|\n",
      "|65.22222222222223|\n",
      "|65.36363636363636|\n",
      "|             72.0|\n",
      "|          64.4375|\n",
      "|74.11764705882354|\n",
      "|70.63157894736842|\n",
      "|          83.3125|\n",
      "|            70.75|\n",
      "|56.88405797101449|\n",
      "|59.79545454545455|\n",
      "|69.90384615384616|\n",
      "|         73.78125|\n",
      "|66.80645161290323|\n",
      "|69.85185185185185|\n",
      "|67.27142857142857|\n",
      "|77.27551020408163|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 统计数据在Type 1分组下 HP的均值\n",
    "sql8 = spark.sql(\"select mean(HP) from Pokemon group by Type1\")\n",
    "sql8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd98afe-5e8d-47e9-b79e-da1d1261a770",
   "metadata": {},
   "source": [
    "---------------------------------------------------\n",
    "\n",
    "#### 任务五：SparkSQL基础语法\n",
    "\n",
    "* 步骤1：学习Spark ML中数据编码模块\n",
    "    * https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html#feature\n",
    "    * https://spark.apache.org/docs/latest/ml-features.html\n",
    "* 步骤2：读取文件Pokemon.csv，理解数据字段含义\n",
    "* 步骤3：将其中的类别属性使用onehotencoder\n",
    "* 步骤4：对其中的数值属性字段使用minmaxscaler\n",
    "* 步骤5：对编码后的属性使用pca进行降维（维度可以自己选择）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b824618f-6a8b-4c3a-bd83-fcbf4d4f4204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/18 00:34:17 WARN SparkContext: The path Pokemon.csv has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('pyspark') \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.addFile('Pokemon.csv')\n",
    "\n",
    "# 在windows下需要将file:// 改为file:///\n",
    "df = spark.read.csv(\"file:///\"+SparkFiles.get(\"Pokemon.csv\"), header=True, inferSchema= True)\n",
    "df = df.withColumnRenamed('Sp. Atk', 'Sp Atk')\n",
    "df = df.withColumnRenamed('Sp. Def', 'Sp Def')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7b0a28-1feb-4d74-8a45-94d20b7e1d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|\n",
      "|          Charmeleon|  Fire|  null|  405| 58|    64|     58|    80|    65|   80|         1|    false|\n",
      "|           Charizard|  Fire|Flying|  534| 78|    84|     78|   109|    85|  100|         1|    false|\n",
      "|CharizardMega Cha...|  Fire|Dragon|  634| 78|   130|    111|   130|    85|  100|         1|    false|\n",
      "|CharizardMega Cha...|  Fire|Flying|  634| 78|   104|     78|   159|   115|  100|         1|    false|\n",
      "|            Squirtle| Water|  null|  314| 44|    48|     65|    50|    64|   43|         1|    false|\n",
      "|           Wartortle| Water|  null|  405| 59|    63|     80|    65|    80|   58|         1|    false|\n",
      "|           Blastoise| Water|  null|  530| 79|    83|    100|    85|   105|   78|         1|    false|\n",
      "|BlastoiseMega Bla...| Water|  null|  630| 79|   103|    120|   135|   115|   78|         1|    false|\n",
      "|            Caterpie|   Bug|  null|  195| 45|    30|     35|    20|    20|   45|         1|    false|\n",
      "|             Metapod|   Bug|  null|  205| 50|    20|     55|    25|    25|   30|         1|    false|\n",
      "|          Butterfree|   Bug|Flying|  395| 60|    45|     50|    90|    80|   70|         1|    false|\n",
      "|              Weedle|   Bug|Poison|  195| 40|    35|     30|    20|    20|   50|         1|    false|\n",
      "|              Kakuna|   Bug|Poison|  205| 45|    25|     50|    25|    25|   35|         1|    false|\n",
      "|            Beedrill|   Bug|Poison|  395| 65|    90|     40|    45|    80|   75|         1|    false|\n",
      "|BeedrillMega Beed...|   Bug|Poison|  495| 65|   150|     40|    15|    80|  145|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 步骤2：读取文件Pokemon.csv，理解数据字段含义\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b51c1-4830-4399-ace7-cedf396fd5ef",
   "metadata": {},
   "source": [
    "该数据为宝可梦中的各生物数据，字段的含义如下：\n",
    "* `Name`: 名字，为String类型；\n",
    "* `Type 1`： 一级分类，类别属性；\n",
    "* `Type 2`：二级分类，类别属性；\n",
    "* `Total`：总数量，数值类型；\n",
    "* `HP`:生命值，数值类型；\n",
    "* `Attack`：普通攻击攻击力，数值类型；\n",
    "* `Defense`：普通防御防御力，数值类型；\n",
    "* `Sp Atk`：特攻攻击力，数值类型；\n",
    "* `Sp Def`：特防防御力，数值类型；\n",
    "* `Speed`：速度，数值类型\n",
    "* `Generation`：宝可梦的不同世代，不同世代，一共有7世代，可以看做是类别属性。\n",
    "* `Legendary`：是否是传奇口袋妖怪，Boolean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d413e92d-68bd-4777-9e4d-889a79f7c9d5",
   "metadata": {},
   "source": [
    "##### 步骤3：将其中的类别属性使用onehotencoder\n",
    "\n",
    "> One-hot 编码将表示为标签索引的分类特征映射到二进制向量，其中最多有一个单值表示所有特征值集中存在特定特征值。\n",
    "> 对于字符串类型的输入数据，通常首先使用 `StringIndexer` 对分类特征进行编码。\n",
    "> OneHotEncoder 可以转换多个列，为每个输入列返回一个单热编码的输出向量列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "51413d6f-c11b-40a0-a7ca-972607cb9838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/17 23:52:25 ERROR Executor: Exception in task 0.0 in stage 325.0 (TID 276)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$4096/0x0000000841584040: (string) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n",
      "\t... 17 more\n",
      "22/03/17 23:52:25 WARN TaskSetManager: Lost task 0.0 in stage 325.0 (TID 276) (192.168.31.58 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$4096/0x0000000841584040: (string) => double)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n",
      "\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n",
      "\t... 17 more\n",
      "\n",
      "22/03/17 23:52:25 ERROR TaskSetManager: Task 0 in stage 325.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o649.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 325.0 failed 1 times, most recent failure: Lost task 0.0 in stage 325.0 (TID 276) (192.168.31.58 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$4096/0x0000000841584040: (string) => double)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat jdk.internal.reflect.GeneratedMethodAccessor95.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$4096/0x0000000841584040: (string) => double)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 17 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [125]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m indexer \u001b[38;5;241m=\u001b[39m StringIndexer(inputCols\u001b[38;5;241m=\u001b[39minputs, outputCols\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[1;32m      8\u001b[0m indexed_df \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mfit(df)\u001b[38;5;241m.\u001b[39mtransform(df)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mindexed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o649.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 325.0 failed 1 times, most recent failure: Lost task 0.0 in stage 325.0 (TID 276) (192.168.31.58 executor driver): org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$4096/0x0000000841584040: (string) => double)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:476)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat jdk.internal.reflect.GeneratedMethodAccessor95.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (StringIndexerModel$$Lambda$4096/0x0000000841584040: (string) => double)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:136)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:349)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "# 分别对Type1 、Type2和generate进行onehotencoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# 先使用StringIndexer对分类特征进行编码\n",
    "inputs = [\"Type 1\", \"Type 2\"]\n",
    "outputs = [\"Type1Index\", \"Type2Index\"]\n",
    "indexer = StringIndexer(inputCols=inputs, outputCols=outputs)\n",
    "indexed_df = indexer.fit(df).transform(df)\n",
    "indexed_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877cc326-2624-44b1-bfc0-b107a1bf9ba0",
   "metadata": {},
   "source": [
    "**直接执行会报错，因为`Type 2`中包含有缺失值，因此需要对缺失值进行处理，可以将其填为一个新的类别“unkown”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "21afec4b-7d6e-4483-87a9-eaddb4777f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna('unkown', subset = \"Type 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "80236f04-5024-43f1-bf35-0ca6735b4421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|\n",
      "|          Charmander|  Fire|unkown|  309| 39|    52|     43|    60|    50|   65|         1|    false|\n",
      "|          Charmeleon|  Fire|unkown|  405| 58|    64|     58|    80|    65|   80|         1|    false|\n",
      "|           Charizard|  Fire|Flying|  534| 78|    84|     78|   109|    85|  100|         1|    false|\n",
      "|CharizardMega Cha...|  Fire|Dragon|  634| 78|   130|    111|   130|    85|  100|         1|    false|\n",
      "|CharizardMega Cha...|  Fire|Flying|  634| 78|   104|     78|   159|   115|  100|         1|    false|\n",
      "|            Squirtle| Water|unkown|  314| 44|    48|     65|    50|    64|   43|         1|    false|\n",
      "|           Wartortle| Water|unkown|  405| 59|    63|     80|    65|    80|   58|         1|    false|\n",
      "|           Blastoise| Water|unkown|  530| 79|    83|    100|    85|   105|   78|         1|    false|\n",
      "|BlastoiseMega Bla...| Water|unkown|  630| 79|   103|    120|   135|   115|   78|         1|    false|\n",
      "|            Caterpie|   Bug|unkown|  195| 45|    30|     35|    20|    20|   45|         1|    false|\n",
      "|             Metapod|   Bug|unkown|  205| 50|    20|     55|    25|    25|   30|         1|    false|\n",
      "|          Butterfree|   Bug|Flying|  395| 60|    45|     50|    90|    80|   70|         1|    false|\n",
      "|              Weedle|   Bug|Poison|  195| 40|    35|     30|    20|    20|   50|         1|    false|\n",
      "|              Kakuna|   Bug|Poison|  205| 45|    25|     50|    25|    25|   35|         1|    false|\n",
      "|            Beedrill|   Bug|Poison|  395| 65|    90|     40|    45|    80|   75|         1|    false|\n",
      "|BeedrillMega Beed...|   Bug|Poison|  495| 65|   150|     40|    15|    80|  145|         1|    false|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "370baa3f-493b-4262-bde4-6d8b8c499623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+----------+----------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|Type1Index|Type2Index|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+----------+----------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|       2.0|       3.0|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|       2.0|       3.0|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|       2.0|       3.0|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|       2.0|       3.0|\n",
      "|          Charmander|  Fire|unkown|  309| 39|    52|     43|    60|    50|   65|         1|    false|       5.0|       0.0|\n",
      "|          Charmeleon|  Fire|unkown|  405| 58|    64|     58|    80|    65|   80|         1|    false|       5.0|       0.0|\n",
      "|           Charizard|  Fire|Flying|  534| 78|    84|     78|   109|    85|  100|         1|    false|       5.0|       1.0|\n",
      "|CharizardMega Cha...|  Fire|Dragon|  634| 78|   130|    111|   130|    85|  100|         1|    false|       5.0|      10.0|\n",
      "|CharizardMega Cha...|  Fire|Flying|  634| 78|   104|     78|   159|   115|  100|         1|    false|       5.0|       1.0|\n",
      "|            Squirtle| Water|unkown|  314| 44|    48|     65|    50|    64|   43|         1|    false|       0.0|       0.0|\n",
      "|           Wartortle| Water|unkown|  405| 59|    63|     80|    65|    80|   58|         1|    false|       0.0|       0.0|\n",
      "|           Blastoise| Water|unkown|  530| 79|    83|    100|    85|   105|   78|         1|    false|       0.0|       0.0|\n",
      "|BlastoiseMega Bla...| Water|unkown|  630| 79|   103|    120|   135|   115|   78|         1|    false|       0.0|       0.0|\n",
      "|            Caterpie|   Bug|unkown|  195| 45|    30|     35|    20|    20|   45|         1|    false|       3.0|       0.0|\n",
      "|             Metapod|   Bug|unkown|  205| 50|    20|     55|    25|    25|   30|         1|    false|       3.0|       0.0|\n",
      "|          Butterfree|   Bug|Flying|  395| 60|    45|     50|    90|    80|   70|         1|    false|       3.0|       1.0|\n",
      "|              Weedle|   Bug|Poison|  195| 40|    35|     30|    20|    20|   50|         1|    false|       3.0|       3.0|\n",
      "|              Kakuna|   Bug|Poison|  205| 45|    25|     50|    25|    25|   35|         1|    false|       3.0|       3.0|\n",
      "|            Beedrill|   Bug|Poison|  395| 65|    90|     40|    45|    80|   75|         1|    false|       3.0|       3.0|\n",
      "|BeedrillMega Beed...|   Bug|Poison|  495| 65|   150|     40|    15|    80|  145|         1|    false|       3.0|       3.0|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 填充空值后再次执行 成功\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# 先使用StringIndexer对分类特征进行编码\n",
    "inputs = [\"Type 1\", \"Type 2\"]\n",
    "outputs = [\"Type1Index\", \"Type2Index\"]\n",
    "indexer = StringIndexer(inputCols=inputs, outputCols=outputs)\n",
    "indexed_df = indexer.fit(df).transform(df)\n",
    "indexed_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b5d1a30f-0cf7-4a53-9321-416ff2b7b3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+----------+----------+--------------+---------------+-------------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|Type1Index|Type2Index|      TypeVec1|       TypeVec2|GenerationVec|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+----------+----------+--------------+---------------+-------------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|       2.0|       3.0|(17,[2],[1.0])| (18,[3],[1.0])|(6,[1],[1.0])|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|       2.0|       3.0|(17,[2],[1.0])| (18,[3],[1.0])|(6,[1],[1.0])|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|       2.0|       3.0|(17,[2],[1.0])| (18,[3],[1.0])|(6,[1],[1.0])|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|       2.0|       3.0|(17,[2],[1.0])| (18,[3],[1.0])|(6,[1],[1.0])|\n",
      "|          Charmander|  Fire|unkown|  309| 39|    52|     43|    60|    50|   65|         1|    false|       5.0|       0.0|(17,[5],[1.0])| (18,[0],[1.0])|(6,[1],[1.0])|\n",
      "|          Charmeleon|  Fire|unkown|  405| 58|    64|     58|    80|    65|   80|         1|    false|       5.0|       0.0|(17,[5],[1.0])| (18,[0],[1.0])|(6,[1],[1.0])|\n",
      "|           Charizard|  Fire|Flying|  534| 78|    84|     78|   109|    85|  100|         1|    false|       5.0|       1.0|(17,[5],[1.0])| (18,[1],[1.0])|(6,[1],[1.0])|\n",
      "|CharizardMega Cha...|  Fire|Dragon|  634| 78|   130|    111|   130|    85|  100|         1|    false|       5.0|      10.0|(17,[5],[1.0])|(18,[10],[1.0])|(6,[1],[1.0])|\n",
      "|CharizardMega Cha...|  Fire|Flying|  634| 78|   104|     78|   159|   115|  100|         1|    false|       5.0|       1.0|(17,[5],[1.0])| (18,[1],[1.0])|(6,[1],[1.0])|\n",
      "|            Squirtle| Water|unkown|  314| 44|    48|     65|    50|    64|   43|         1|    false|       0.0|       0.0|(17,[0],[1.0])| (18,[0],[1.0])|(6,[1],[1.0])|\n",
      "|           Wartortle| Water|unkown|  405| 59|    63|     80|    65|    80|   58|         1|    false|       0.0|       0.0|(17,[0],[1.0])| (18,[0],[1.0])|(6,[1],[1.0])|\n",
      "|           Blastoise| Water|unkown|  530| 79|    83|    100|    85|   105|   78|         1|    false|       0.0|       0.0|(17,[0],[1.0])| (18,[0],[1.0])|(6,[1],[1.0])|\n",
      "|BlastoiseMega Bla...| Water|unkown|  630| 79|   103|    120|   135|   115|   78|         1|    false|       0.0|       0.0|(17,[0],[1.0])| (18,[0],[1.0])|(6,[1],[1.0])|\n",
      "|            Caterpie|   Bug|unkown|  195| 45|    30|     35|    20|    20|   45|         1|    false|       3.0|       0.0|(17,[3],[1.0])| (18,[0],[1.0])|(6,[1],[1.0])|\n",
      "|             Metapod|   Bug|unkown|  205| 50|    20|     55|    25|    25|   30|         1|    false|       3.0|       0.0|(17,[3],[1.0])| (18,[0],[1.0])|(6,[1],[1.0])|\n",
      "|          Butterfree|   Bug|Flying|  395| 60|    45|     50|    90|    80|   70|         1|    false|       3.0|       1.0|(17,[3],[1.0])| (18,[1],[1.0])|(6,[1],[1.0])|\n",
      "|              Weedle|   Bug|Poison|  195| 40|    35|     30|    20|    20|   50|         1|    false|       3.0|       3.0|(17,[3],[1.0])| (18,[3],[1.0])|(6,[1],[1.0])|\n",
      "|              Kakuna|   Bug|Poison|  205| 45|    25|     50|    25|    25|   35|         1|    false|       3.0|       3.0|(17,[3],[1.0])| (18,[3],[1.0])|(6,[1],[1.0])|\n",
      "|            Beedrill|   Bug|Poison|  395| 65|    90|     40|    45|    80|   75|         1|    false|       3.0|       3.0|(17,[3],[1.0])| (18,[3],[1.0])|(6,[1],[1.0])|\n",
      "|BeedrillMega Beed...|   Bug|Poison|  495| 65|   150|     40|    15|    80|  145|         1|    false|       3.0|       3.0|(17,[3],[1.0])| (18,[3],[1.0])|(6,[1],[1.0])|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+----------+----------+--------------+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用新生成的index列进行Onehot编码\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"Type1Index\", \"Type2Index\",\"Generation\"],\n",
    "                        outputCols=[\"TypeVec1\", \"TypeVec2\",\"GenerationVec\"])\n",
    "model = encoder.fit(indexed_df)\n",
    "encoded = model.transform(indexed_df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd6e1e-3163-4161-b714-f83888336394",
   "metadata": {},
   "source": [
    "##### 步骤4：对其中的数值属性字段使用minmaxscaler\n",
    "\n",
    "* `Total`\n",
    "* `HP`\n",
    "* `Attack`\n",
    "* `Defense`\n",
    "* `Sp Atk`\n",
    "* `Sp Def`\n",
    "* `Speed`\n",
    "\n",
    "> `MinMaxScaler` 转换向量行的数据集，将每个特征重新缩放到特定范围（通常为 $[0, 1]$）。它接受参数： `min`：默认为 `0.0`。转换后的下界，由所有特征共享。 `max`：默认为 `1.0`。转换后的上限，由所有特征共享。\n",
    "\n",
    "> MinMaxScaler 计算数据集的汇总统计数据并生成 MinMaxScalerModel。然后，模型可以单独转换每个特征，使其在给定范围内。\n",
    "\n",
    "特征 E 的重新缩放值计算为:\n",
    "\n",
    "$$\\begin{equation} Rescaled(e_i) = \\frac{e_i - E_{min}}{E_{max} - E_{min}} * (max - min) + min \\end{equation}$$\n",
    "\n",
    "其中，$E_{max} == E_{min}\\ $,    $\\ \\ \\ Rescaled(e_i) = 0.5 * (max + min)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca00f1b-f775-4e84-bee5-e67ce04e98f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+--------------------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|            features|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+--------------------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|[318.0,45.0,49.0,...|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|[405.0,60.0,62.0,...|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|[525.0,80.0,82.0,...|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|[625.0,80.0,100.0...|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|[309.0,39.0,52.0,...|\n",
      "|          Charmeleon|  Fire|  null|  405| 58|    64|     58|    80|    65|   80|         1|    false|[405.0,58.0,64.0,...|\n",
      "|           Charizard|  Fire|Flying|  534| 78|    84|     78|   109|    85|  100|         1|    false|[534.0,78.0,84.0,...|\n",
      "|CharizardMega Cha...|  Fire|Dragon|  634| 78|   130|    111|   130|    85|  100|         1|    false|[634.0,78.0,130.0...|\n",
      "|CharizardMega Cha...|  Fire|Flying|  634| 78|   104|     78|   159|   115|  100|         1|    false|[634.0,78.0,104.0...|\n",
      "|            Squirtle| Water|  null|  314| 44|    48|     65|    50|    64|   43|         1|    false|[314.0,44.0,48.0,...|\n",
      "|           Wartortle| Water|  null|  405| 59|    63|     80|    65|    80|   58|         1|    false|[405.0,59.0,63.0,...|\n",
      "|           Blastoise| Water|  null|  530| 79|    83|    100|    85|   105|   78|         1|    false|[530.0,79.0,83.0,...|\n",
      "|BlastoiseMega Bla...| Water|  null|  630| 79|   103|    120|   135|   115|   78|         1|    false|[630.0,79.0,103.0...|\n",
      "|            Caterpie|   Bug|  null|  195| 45|    30|     35|    20|    20|   45|         1|    false|[195.0,45.0,30.0,...|\n",
      "|             Metapod|   Bug|  null|  205| 50|    20|     55|    25|    25|   30|         1|    false|[205.0,50.0,20.0,...|\n",
      "|          Butterfree|   Bug|Flying|  395| 60|    45|     50|    90|    80|   70|         1|    false|[395.0,60.0,45.0,...|\n",
      "|              Weedle|   Bug|Poison|  195| 40|    35|     30|    20|    20|   50|         1|    false|[195.0,40.0,35.0,...|\n",
      "|              Kakuna|   Bug|Poison|  205| 45|    25|     50|    25|    25|   35|         1|    false|[205.0,45.0,25.0,...|\n",
      "|            Beedrill|   Bug|Poison|  395| 65|    90|     40|    45|    80|   75|         1|    false|[395.0,65.0,90.0,...|\n",
      "|BeedrillMega Beed...|   Bug|Poison|  495| 65|   150|     40|    15|    80|  145|         1|    false|[495.0,65.0,150.0...|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 因为MinMaxScaler接受的是Vector类型，因此将每一列转换为vector的形式\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "num_columns = ['Total','HP','Attack','Defense','Sp Atk','Sp Def','Speed']\n",
    "num_columns_new = ['TotalNew','HPNew','AttackNew','DefenseNew','Sp AtkNew','Sp DefNew','SpeedNew']\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=num_columns, outputCol='features')\n",
    "output = vecAssembler.transform(df)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7d43b91-c730-4b01-a750-fbfeb8ad739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+-------+--------------------+-------+--------------------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary| c2_new|               mm_c2|c2_new_|              mm_c2_|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+-------+--------------------+-------+--------------------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|[318.0]|              [0.23]|[318.0]|              [0.23]|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|[405.0]|             [0.375]|[405.0]|             [0.375]|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|[525.0]|[0.5750000000000001]|[525.0]|[0.5750000000000001]|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|[625.0]|[0.7416666666666667]|[625.0]|[0.7416666666666667]|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|[309.0]|[0.21500000000000...|[309.0]|[0.21500000000000...|\n",
      "|          Charmeleon|  Fire|  null|  405| 58|    64|     58|    80|    65|   80|         1|    false|[405.0]|             [0.375]|[405.0]|             [0.375]|\n",
      "|           Charizard|  Fire|Flying|  534| 78|    84|     78|   109|    85|  100|         1|    false|[534.0]|[0.5900000000000001]|[534.0]|[0.5900000000000001]|\n",
      "|CharizardMega Cha...|  Fire|Dragon|  634| 78|   130|    111|   130|    85|  100|         1|    false|[634.0]|[0.7566666666666667]|[634.0]|[0.7566666666666667]|\n",
      "|CharizardMega Cha...|  Fire|Flying|  634| 78|   104|     78|   159|   115|  100|         1|    false|[634.0]|[0.7566666666666667]|[634.0]|[0.7566666666666667]|\n",
      "|            Squirtle| Water|  null|  314| 44|    48|     65|    50|    64|   43|         1|    false|[314.0]|[0.22333333333333...|[314.0]|[0.22333333333333...|\n",
      "|           Wartortle| Water|  null|  405| 59|    63|     80|    65|    80|   58|         1|    false|[405.0]|             [0.375]|[405.0]|             [0.375]|\n",
      "|           Blastoise| Water|  null|  530| 79|    83|    100|    85|   105|   78|         1|    false|[530.0]|[0.5833333333333334]|[530.0]|[0.5833333333333334]|\n",
      "|BlastoiseMega Bla...| Water|  null|  630| 79|   103|    120|   135|   115|   78|         1|    false|[630.0]|              [0.75]|[630.0]|              [0.75]|\n",
      "|            Caterpie|   Bug|  null|  195| 45|    30|     35|    20|    20|   45|         1|    false|[195.0]|             [0.025]|[195.0]|             [0.025]|\n",
      "|             Metapod|   Bug|  null|  205| 50|    20|     55|    25|    25|   30|         1|    false|[205.0]|[0.04166666666666...|[205.0]|[0.04166666666666...|\n",
      "|          Butterfree|   Bug|Flying|  395| 60|    45|     50|    90|    80|   70|         1|    false|[395.0]|[0.35833333333333...|[395.0]|[0.35833333333333...|\n",
      "|              Weedle|   Bug|Poison|  195| 40|    35|     30|    20|    20|   50|         1|    false|[195.0]|             [0.025]|[195.0]|             [0.025]|\n",
      "|              Kakuna|   Bug|Poison|  205| 45|    25|     50|    25|    25|   35|         1|    false|[205.0]|[0.04166666666666...|[205.0]|[0.04166666666666...|\n",
      "|            Beedrill|   Bug|Poison|  395| 65|    90|     40|    45|    80|   75|         1|    false|[395.0]|[0.35833333333333...|[395.0]|[0.35833333333333...|\n",
      "|BeedrillMega Beed...|   Bug|Poison|  495| 65|   150|     40|    15|    80|  145|         1|    false|[495.0]|             [0.525]|[495.0]|             [0.525]|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+-------+--------------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "# 首先将c2列转换为vector的形式\n",
    "vecAssembler = VectorAssembler(inputCols=[\"Total\"], outputCol=\"c2_new_\")\n",
    "# minmax tranform\n",
    "mmScaler = MinMaxScaler(inputCol='c2_new_', outputCol='mm_c2_')\n",
    "pipeline = Pipeline(stages=[vecAssembler, mmScaler])\n",
    "pipeline_fit = pipeline.fit(df)\n",
    "df = pipeline_fit.transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "643fb402-e955-4032-88ee-c9ab83c7bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                Name|Type 1|Type 2|Total| HP|Attack|Defense|Sp Atk|Sp Def|Speed|Generation|Legendary|         scaledTotal|            scaledHP|        scaledAttack|       scaledDefense|        scaledSp Atk|        scaledSp Def|         scaledSpeed|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|           Bulbasaur| Grass|Poison|  318| 45|    49|     49|    65|    65|   45|         1|    false|              [0.23]|[0.1732283464566929]|[0.23783783783783...|[0.19555555555555...|[0.29891304347826...|[0.2142857142857143]|[0.22857142857142...|\n",
      "|             Ivysaur| Grass|Poison|  405| 60|    62|     63|    80|    80|   60|         1|    false|             [0.375]|[0.23228346456692...|[0.3081081081081081]|[0.2577777777777778]|[0.3804347826086956]|[0.28571428571428...|[0.3142857142857143]|\n",
      "|            Venusaur| Grass|Poison|  525| 80|    82|     83|   100|   100|   80|         1|    false|[0.5750000000000001]|[0.3110236220472441]|[0.41621621621621...|[0.3466666666666667]|[0.4891304347826087]| [0.380952380952381]|[0.42857142857142...|\n",
      "|VenusaurMega Venu...| Grass|Poison|  625| 80|   100|    123|   122|   120|   80|         1|    false|[0.7416666666666667]|[0.3110236220472441]|[0.5135135135135136]|[0.5244444444444445]|[0.6086956521739131]|[0.4761904761904762]|[0.42857142857142...|\n",
      "|          Charmander|  Fire|  null|  309| 39|    52|     43|    60|    50|   65|         1|    false|[0.21500000000000...|[0.14960629921259...|[0.25405405405405...|[0.1688888888888889]|[0.2717391304347826]|[0.14285714285714...|[0.34285714285714...|\n",
      "|          Charmeleon|  Fire|  null|  405| 58|    64|     58|    80|    65|   80|         1|    false|             [0.375]|[0.22440944881889...|[0.31891891891891...|[0.23555555555555...|[0.3804347826086956]|[0.2142857142857143]|[0.42857142857142...|\n",
      "|           Charizard|  Fire|Flying|  534| 78|    84|     78|   109|    85|  100|         1|    false|[0.5900000000000001]|[0.3031496062992126]|[0.42702702702702...|[0.3244444444444444]|[0.5380434782608695]|[0.30952380952380...|[0.5428571428571428]|\n",
      "|CharizardMega Cha...|  Fire|Dragon|  634| 78|   130|    111|   130|    85|  100|         1|    false|[0.7566666666666667]|[0.3031496062992126]|[0.6756756756756757]|[0.4711111111111111]|[0.6521739130434783]|[0.30952380952380...|[0.5428571428571428]|\n",
      "|CharizardMega Cha...|  Fire|Flying|  634| 78|   104|     78|   159|   115|  100|         1|    false|[0.7566666666666667]|[0.3031496062992126]|[0.5351351351351352]|[0.3244444444444444]|[0.8097826086956521]|[0.45238095238095...|[0.5428571428571428]|\n",
      "|            Squirtle| Water|  null|  314| 44|    48|     65|    50|    64|   43|         1|    false|[0.22333333333333...|[0.16929133858267...|[0.23243243243243...|[0.26666666666666...|[0.21739130434782...|[0.20952380952380...|[0.21714285714285...|\n",
      "|           Wartortle| Water|  null|  405| 59|    63|     80|    65|    80|   58|         1|    false|             [0.375]|[0.22834645669291...|[0.31351351351351...|[0.3333333333333333]|[0.29891304347826...|[0.28571428571428...|[0.3028571428571429]|\n",
      "|           Blastoise| Water|  null|  530| 79|    83|    100|    85|   105|   78|         1|    false|[0.5833333333333334]|[0.30708661417322...|[0.42162162162162...|[0.4222222222222222]|[0.4076086956521739]|[0.4047619047619048]|[0.41714285714285...|\n",
      "|BlastoiseMega Bla...| Water|  null|  630| 79|   103|    120|   135|   115|   78|         1|    false|              [0.75]|[0.30708661417322...|[0.5297297297297298]|[0.5111111111111111]|[0.6793478260869565]|[0.45238095238095...|[0.41714285714285...|\n",
      "|            Caterpie|   Bug|  null|  195| 45|    30|     35|    20|    20|   45|         1|    false|             [0.025]|[0.1732283464566929]|[0.13513513513513...|[0.13333333333333...|[0.05434782608695...|               [0.0]|[0.22857142857142...|\n",
      "|             Metapod|   Bug|  null|  205| 50|    20|     55|    25|    25|   30|         1|    false|[0.04166666666666...|[0.19291338582677...|[0.08108108108108...|[0.2222222222222222]|[0.08152173913043...|[0.02380952380952...|[0.14285714285714...|\n",
      "|          Butterfree|   Bug|Flying|  395| 60|    45|     50|    90|    80|   70|         1|    false|[0.35833333333333...|[0.23228346456692...|[0.21621621621621...|               [0.2]|[0.43478260869565...|[0.28571428571428...|[0.37142857142857...|\n",
      "|              Weedle|   Bug|Poison|  195| 40|    35|     30|    20|    20|   50|         1|    false|             [0.025]|[0.15354330708661...|[0.16216216216216...|[0.1111111111111111]|[0.05434782608695...|               [0.0]|[0.2571428571428571]|\n",
      "|              Kakuna|   Bug|Poison|  205| 45|    25|     50|    25|    25|   35|         1|    false|[0.04166666666666...|[0.1732283464566929]|[0.10810810810810...|               [0.2]|[0.08152173913043...|[0.02380952380952...|[0.17142857142857...|\n",
      "|            Beedrill|   Bug|Poison|  395| 65|    90|     40|    45|    80|   75|         1|    false|[0.35833333333333...|[0.25196850393700...|[0.4594594594594595]|[0.15555555555555...|[0.1902173913043478]|[0.28571428571428...|               [0.4]|\n",
      "|BeedrillMega Beed...|   Bug|Poison|  495| 65|   150|     40|    15|    80|  145|         1|    false|             [0.525]|[0.25196850393700...|[0.7837837837837838]|[0.15555555555555...|[0.02717391304347...|[0.28571428571428...|               [0.8]|\n",
      "+--------------------+------+------+-----+---+------+-------+------+------+-----+----------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "copy_df = df\n",
    "for numC in num_columns:\n",
    "    # 因为MinMaxScaler接受的是Vector类型，因此将每一列转换为vector的形式\n",
    "    vecAssembler = VectorAssembler(inputCols=[numC], outputCol=numC+'features')\n",
    "    # 使用minMax转换\n",
    "    scaler = MinMaxScaler(inputCol=numC+'features', outputCol=\"scaled\" + numC)\n",
    "    pipeline = Pipeline(stages=[vecAssembler, scaler])\n",
    "    pipeline_fit = pipeline.fit(copy_df)\n",
    "    copy_df = pipeline_fit.transform(copy_df)\n",
    "    # 删除中间Vectoer列\n",
    "    copy_df = copy_df.drop(numC+'features')\n",
    "copy_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e0013-d868-44e1-a63f-920857f67455",
   "metadata": {},
   "source": [
    "##### 步骤5：对编码后的属性使用pca进行降维（维度可以自己选择）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c80c34-2e45-4758-898c-f4860df8a052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
